# Hyperscan

Hyperscan是一个来自于Intel的高性能的正则表达式匹配引擎。具有同时匹配大规模规则的强大能力，并展现了出色的匹配性能与扩展性。同时，它针对网络报文处理设计了独有的流模式匹配模式。



以自动机理论为基础，其工作流程主要分成两个部分：编译期和运行期。



编译期：

Hyperscan 自带C++编写的正则表达式编译器。将正则表达式作为输入，针对不同的用户定义的模式（流模式）及特殊语法，经过复杂的图分析及优化过程，生成对应的hyperscan数据库，生成的数据库可以被序列化后保存在内存中，以供运行期提取使用。



运行期：

用户需要预先分配一段内存来存储临时匹配状态信息，之后利用编译生成的数据库调用Hyperscan内部的匹配引擎(NFA, DFA等)来对输入进行模式匹配。Hyperscan在引擎中使用Intel处理器所具有的SIMD指令进行加速。同时，用户可以通过回调函数来自定义匹配发生后采取的行为。由于生成的数据库是只读的，用户可以在多个CPU核或多线程场景下共享数据库来提升匹配扩展性。

SIMD单指令流多数据流(SingleInstruction Multiple Data,SIMD)是一种采用一个控制器来控制多个处理器，同时对一组数据（又称“数据向量”）中的每一个分别执行相同的操作从而实现空间上的并行性的技术。



大规模匹配和高性能：

根据规则复杂度的不同，Hyperscan能支持几万到几十万的规则的匹配。与传统正则匹配引擎不同，Hyperscan支持多规则的同步匹配。在用户为每条规则指定独有的编号后，Hypercan可以将所有规则编译成一个数据库并在匹配过程中输出所有当前匹配到的规则信息。

Hyperscan具有良好的扩展性，随着使用核数的增加，匹配性能基本处于线性增长的趋势。



流模式：

流模式是Hyperscan为网络场景下跨报文匹配设计的特殊匹配模式。在真实网络场景下，数据是分散在多报文中。若有数据在尚未到达的报文中时，传统匹配模式将无法适用。在流模式下，Hyperscan可以保存当前数据匹配的状态，并以其作为接收到新数据时的初始匹配状态。另外，Hyperscan对保存的匹配状态进行了压缩以减少流模式对内存的占用。



   匹配就是基于编译好的database，对数据进行匹配，并得到匹配结果。 hyperscan在进行匹配时需要有一个临时数据(scratch），这块数据需要在数据面运行前就分配（不在运行时分配和释放，保证性能），且需要保证同一时刻仅有一个匹配过程在使用同一块临时数据。如果使用流模式，还需要预先为每个流分配流状态数据。每个工作线程可以共享一个database，但是都需要有自己的临时空间。

​    匹配结果是通过用户自定义的回调函数获取的。匹配过程中只要发生命中，就会调用此函数。用户通过实参，可以获得命中正则表达式对应的ID、匹配位置等信息。



# AC自动机

KMP和trie树，KMP是用于一对一的字符串匹配,而trie虽然能用于多模式匹配,但是每次匹配失败都需要进行回溯,如果模式串很长的话会很浪费时间,所以AC自动机应运而生。



ac自动机,就是在tire树的基础上,增加一个fail指针,如果当前点匹配失败,则将指针转移到fail指针指向的地方,这样就不用回溯,而可以匹配下去了.

fail指针的构建都是用BFS实现的。

对于直接与根节点相连的节点：他们的Fail指针直接指向root。

其他节点其Fail指针求法如下：假设当前节点为father，其孩子节点记为child。求child的Fail指针时，首先我们要找到其father的Fail指针所指向的节点,假如是t的话，我们就要看t的孩子中有没有和child节点所表示的字母相同的节点，如果有的话，这个节点就是child的fail指针，如果发现没有，则需要找father->fail->fail这个节点，然后重复上面过程，如果一直找都找不到，则child的Fail指针就要指向root。



一个构建失败指针例子：



![img](.\图片\114f15df78e4431fb383f146a1c48673.jpg)

首先我们将root入队。

第零层：root节点

第一层（fail为红色虚线）：

root出队，遍历root所有的子节点h，s使之fail指向root，并且将h，s入队。

第二层（fail为蓝色虚线）：

1.h出队，找到e，找到h节点的失败节点root，root节点下无e节点，**因为节点为root**，e的fail指向root，e入队。

2.s出队，

找到a，找到s节点的失败节点root，root节点下无a节点，**因为节点为root**，a的fail指向root，a入队。

找到h，找到s节点的失败节点root，**root下有h节点**，第二层的h节点的fail指向root节点的子节点h。h入队。

第三层（fail为绿色虚线）：

1.e出队，找到r，找到e节点的失败节点root，root节点下无r节点，**因为节点为root**，r的fail指向root，r入队。

2.a出队，找到y，找到a节点的失败节点root，root节点下无y节点，**因为节点为root**，y的fail指向root，y入队。

3.h出队，找到e，找到h节点（第二层）的失败节点h节点（第一层），h节点下有e节点，则当前e节点的fail指针指向第二层的e节点。e入队。

找到r，找到h节点（第二层）的失败节点h节点（第一层），h节点下没有r节点，然后找h节点（第一层）的fail指针，是root，则当前r节点的fail指针指向root。r入队。



一次匹配的例子：用上面已经构造好的 AC 自动机

我们现在用目标串：yasherhs 进行匹配，设匹配节点 p，初始节点 p=root，当 i=0,1 时匹配不成功。p 依旧指向 root。

当 i=2，3，4 时，p 节点匹配路径为 she，p 指向了第三层的 e 节点，计数一次 (1)。

现在构建临时的一个 temp 节点，temp=p, 去找 temp->fail，发现 temp->fail 也为结尾，计数一次 (2)。继续找 temp->fail->fail, 找到 root 结束。

当 i=5 时，继续寻找 p 节点的子节点，没找到 r 节点，p=p->fail，我们找到了第二层的 e 节点，找到子节点 r，p=r，r 为结尾节点，计数一次 (3)。

当 i=6 时，未找到 h 子节点，p 回到了 r->fail 节点 root。

当 i=7 时，找到 h 节点但并不是结尾节点。

i=8 回到 root。结束。



最后成功匹配的三次，在匹配的时候对具体代码进行调整，也实现每个模式串只匹配一次，记录匹配成功的串等操作。



*经典的 AC 自动机是判断模式串出现的个数，因此在每次成功的时候会对节点进行已访问的标记。*



# rabbitmq

**消息队列的作用与使用场景**

异步处理：批量数据异步处理（批量上传文件）

流量削峰：高负载任务负载均衡（电商秒杀抢购）

应用解耦：串行任务并行化（退货流程解耦）

消息分发：基于Pub/Sub实现一对多通信

消息缓存

注：Kafka 定位是日志消息队列。吞吐量最大。阿里的Rocket MQ ,Rabbit MQ 是可靠性更强，对数据一致性、稳定性和可靠性要求很高的场景。劣势是Rabbit mQ 的性能，吞吐量不如Rocket MQ 高。



**如何确保消息正确地发送至 RabbitMQ？**

RabbitMQ 使用发送方确认模式，确保消息正确地发送到 RabbitMQ。

发送方确认模式：将信道设置成 confirm 模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一的 ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后（可持久化的消息），信道会发送一个确认给生产者（包含消息唯一 ID）。如果 RabbitMQ 发生内部错误从而导致消息丢失，会发送一条 nack（not acknowledged，未确认）消息。

发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。当确认消息到达生产者应用程序，生产者应用程序的回调方法就会被触发来处理确认消息。



**如何确保消息接收方消费了消息？**

接收方消息确认机制：消费者接收每一条消息后都必须进行确认（消息接收和消息确认是两个不同操作）。只有消费者确认了消息，RabbitMQ 才能安全地把消息从队列中删除。

这里并没有用到超时机制，RabbitMQ 仅通过 Consumer 的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ 给了 Consumer 足够长的时间来处理消息。

下面罗列几种特殊情况：

- 如果消费者接收到消息，在确认之前断开了连接或取消订阅，RabbitMQ 会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消息重复消费的隐患，需要根据 bizId 去重）
- 如果消费者接收到消息却没有确认消息，连接也未断开，则 RabbitMQ 认为该消费者繁忙，将不会给该消费者分发更多的消息。



**如何避免消息重复投递或重复消费？**

在消息生产时，MQ 内部针对每条生产者发送的消息生成一个 inner-msg-id，作为去重和幂等的依据（消息投递失败并重传），避免重复的消息进入队列；在消息消费时，要求消息体中必须要有一个 bizId（对于同一业务全局唯一，如支付 ID、订单 ID、帖子 ID 等）作为去重和幂等的依据，避免同一条消息被重复消费。



 **消息基于什么传输？**

由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。



**消息如何分发？**

若该队列至少有一个消费者订阅，消息将以循环（round-robin）的方式发送给消费者。每条消息只会分发给一个订阅的消费者（前提是消费者能够正常处理消息并进行确认）。



**如何确保消息不丢失？**

消息持久化的前提是：将交换器 / 队列的 durable 属性设置为 true，表示交换器 / 队列是持久交换器 / 队列，在服务器崩溃或重启之后不需要重新创建交换器 / 队列（交换器 / 队列会自动创建）。

如果消息想要从 Rabbit 崩溃中恢复，那么消息必须：

- 在消息发布前，通过把它的 “投递模式” 选项设置为 2（持久）来把消息标记成持久化
- 将消息发送到持久交换器
- 消息到达持久队列

RabbitMQ 确保持久性消息能从服务器重启中恢复的方式是，将它们写入磁盘上的一个持久化日志文件，当发布一条持久性消息到持久交换器上时，Rabbit 会在消息提交到日志文件后才发送响应（如果消息路由到了非持久队列，它会自动从持久化日志中移除）。一旦消费者从持久队列中消费了一条持久化消息，RabbitMQ 会在持久化日志中把这条消息标记为等待垃圾收集。如果持久化消息在被消费之前 RabbitMQ 重启，那么 Rabbit 会自动重建交换器和队列（以及绑定），并重播持久化日志文件中的消息到合适的队列或者交换器上。



**RabbitMQ 的 channel、exchange 和 queue**

queue 具有自己的 erlang 进程；

exchange 内部实现为保存 binding 关系的查找表；

channel 是实际进行路由工作的实体，即负责按照 routing_key 将 message 投递给 queue 。投递到特定 channel 上的 message 是有顺序的。



Redis

